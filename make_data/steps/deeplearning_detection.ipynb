{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntr3kXD8Kg3o"
      },
      "source": [
        "### Please run with Google Colab with Good GPU\n",
        "<a href=\"https://colab.research.google.com/github/wakachii/SI-Org-chart/blob/main/make_data/steps/deeplearning_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P3gTVMDKg3q",
        "outputId": "cd04abcf-fb69-4d53-f436-a7000187028d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-lznc1om_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-lznc1om_\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 9604f5995cc628619f0e4fd913453b4d7d61db3f\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.5.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.1.8)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp311-cp311-linux_x86_64.whl size=6380945 sha256=def9937faf828de027c37765ca16b6ea67ebdc0be86c53afb6db453cbcc88d8c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f5vtwc9z/wheels/17/d9/40/60db98e485aa9455d653e29d1046601ce96fe23647f60c1c5a\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=014e1cf5036ce3bc36b2b6b82220c49f71d9127d34e0f541c3dec4fc57c73bf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=8cfa1700eed027da8941d24f0fcd651e98e55a7286c26369ebade7ca52d5d4b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "# Detectron2 has not released pre-built binaries for the latest pytorch (https://github.com/facebookresearch/detectron2/issues/4053)\n",
        "# so we install from source instead. This takes a few minutes.\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDqe8fW1Kg3q",
        "outputId": "786abbbe-3693-4136-c812-26de5f279866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import cv2 as cv2\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "import detectron2\n",
        "from tqdm import tqdm\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_FTuJydKg3r",
        "outputId": "1b542867-43ea-449e-b274-85dbd7a63093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/22 06:35:02 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[02/22 06:35:05 d2.data.datasets.coco]: Loading /content/drive/MyDrive/scan_org_charts/data/Org_chart/learning/Organization_annotation.json takes 2.67 seconds.\n",
            "WARNING [02/22 06:35:05 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[02/22 06:35:05 d2.data.datasets.coco]: Loaded 50 images in COCO format from /content/drive/MyDrive/scan_org_charts/data/Org_chart/learning/Organization_annotation.json\n",
            "[02/22 06:35:05 d2.data.build]: Removed 0 images with no usable annotations. 50 images left.\n",
            "[02/22 06:35:05 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "| department | 1338         |    info    | 70           |\n",
            "|            |              |            |              |\n",
            "|   total    | 1408         |            |              |\n",
            "[02/22 06:35:05 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[02/22 06:35:05 d2.data.build]: Using training sampler TrainingSampler\n",
            "[02/22 06:35:05 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[02/22 06:35:05 d2.data.common]: Serializing 50 elements to byte tensors and concatenating them all ...\n",
            "[02/22 06:35:05 d2.data.common]: Serialized dataset takes 0.16 MiB\n",
            "[02/22 06:35:05 d2.data.build]: Making batched data loader with batch_size=1\n",
            "WARNING [02/22 06:35:05 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "[02/22 06:35:05 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f10217.pkl: 178MB [00:00, 232MB/s]                           \n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (4, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/22 06:35:06 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/22 06:35:20 d2.utils.events]:  eta: 0:01:59  iter: 19  total_loss: 5.99  loss_cls: 1.527  loss_box_reg: 0.618  loss_mask: 0.6935  loss_rpn_cls: 2.659  loss_rpn_loc: 0.3809    time: 0.5001  last_time: 0.6029  data_time: 0.3299  last_data_time: 0.4343   lr: 1.5585e-05  max_mem: 1422M\n",
            "[02/22 06:35:34 d2.utils.events]:  eta: 0:01:54  iter: 39  total_loss: 3.143  loss_cls: 1.306  loss_box_reg: 0.7158  loss_mask: 0.647  loss_rpn_cls: 0.2173  loss_rpn_loc: 0.2904    time: 0.5025  last_time: 0.4632  data_time: 0.3168  last_data_time: 0.3039   lr: 3.1569e-05  max_mem: 1422M\n",
            "[02/22 06:35:44 d2.utils.events]:  eta: 0:02:43  iter: 59  total_loss: 2.685  loss_cls: 0.9342  loss_box_reg: 0.8217  loss_mask: 0.5685  loss_rpn_cls: 0.08731  loss_rpn_loc: 0.2139    time: 0.4864  last_time: 0.4411  data_time: 0.1820  last_data_time: 0.1444   lr: 4.7553e-05  max_mem: 1422M\n",
            "[02/22 06:35:48 d2.utils.events]:  eta: 0:01:56  iter: 79  total_loss: 2.248  loss_cls: 0.6795  loss_box_reg: 0.7255  loss_mask: 0.4513  loss_rpn_cls: 0.05963  loss_rpn_loc: 0.2179    time: 0.4233  last_time: 0.2233  data_time: 0.0066  last_data_time: 0.0032   lr: 6.3537e-05  max_mem: 1422M\n",
            "[02/22 06:35:53 d2.utils.events]:  eta: 0:01:45  iter: 99  total_loss: 1.883  loss_cls: 0.4858  loss_box_reg: 0.7097  loss_mask: 0.3846  loss_rpn_cls: 0.08682  loss_rpn_loc: 0.1862    time: 0.3879  last_time: 0.2895  data_time: 0.0074  last_data_time: 0.0109   lr: 7.9521e-05  max_mem: 1422M\n",
            "[02/22 06:35:59 d2.utils.events]:  eta: 0:01:40  iter: 119  total_loss: 1.79  loss_cls: 0.422  loss_box_reg: 0.7626  loss_mask: 0.3416  loss_rpn_cls: 0.06357  loss_rpn_loc: 0.1793    time: 0.3708  last_time: 0.1734  data_time: 0.0374  last_data_time: 0.0052   lr: 9.5505e-05  max_mem: 1422M\n",
            "[02/22 06:36:04 d2.utils.events]:  eta: 0:01:33  iter: 139  total_loss: 1.661  loss_cls: 0.3616  loss_box_reg: 0.6781  loss_mask: 0.3222  loss_rpn_cls: 0.07452  loss_rpn_loc: 0.2214    time: 0.3515  last_time: 0.1919  data_time: 0.0165  last_data_time: 0.0043   lr: 0.00011149  max_mem: 1422M\n",
            "[02/22 06:36:09 d2.utils.events]:  eta: 0:01:28  iter: 159  total_loss: 1.656  loss_cls: 0.3579  loss_box_reg: 0.7006  loss_mask: 0.2871  loss_rpn_cls: 0.0422  loss_rpn_loc: 0.1868    time: 0.3419  last_time: 0.3134  data_time: 0.0151  last_data_time: 0.0098   lr: 0.00012747  max_mem: 1422M\n",
            "[02/22 06:36:14 d2.utils.events]:  eta: 0:01:22  iter: 179  total_loss: 1.524  loss_cls: 0.3285  loss_box_reg: 0.6583  loss_mask: 0.3042  loss_rpn_cls: 0.04724  loss_rpn_loc: 0.165    time: 0.3315  last_time: 0.1838  data_time: 0.0119  last_data_time: 0.0151   lr: 0.00014346  max_mem: 1422M\n",
            "[02/22 06:36:19 d2.utils.events]:  eta: 0:01:15  iter: 199  total_loss: 1.397  loss_cls: 0.2699  loss_box_reg: 0.585  loss_mask: 0.2649  loss_rpn_cls: 0.0414  loss_rpn_loc: 0.173    time: 0.3215  last_time: 0.2213  data_time: 0.0235  last_data_time: 0.0038   lr: 0.00015944  max_mem: 1422M\n",
            "[02/22 06:36:25 d2.utils.events]:  eta: 0:01:12  iter: 219  total_loss: 1.361  loss_cls: 0.3107  loss_box_reg: 0.5275  loss_mask: 0.2659  loss_rpn_cls: 0.03851  loss_rpn_loc: 0.1879    time: 0.3196  last_time: 0.1985  data_time: 0.0359  last_data_time: 0.0015   lr: 0.00017542  max_mem: 1422M\n",
            "[02/22 06:36:30 d2.utils.events]:  eta: 0:01:06  iter: 239  total_loss: 1.281  loss_cls: 0.2487  loss_box_reg: 0.5425  loss_mask: 0.2208  loss_rpn_cls: 0.03977  loss_rpn_loc: 0.1966    time: 0.3120  last_time: 0.2874  data_time: 0.0141  last_data_time: 0.0211   lr: 0.00019141  max_mem: 1422M\n",
            "[02/22 06:36:34 d2.utils.events]:  eta: 0:01:00  iter: 259  total_loss: 1.25  loss_cls: 0.25  loss_box_reg: 0.4903  loss_mask: 0.2215  loss_rpn_cls: 0.04073  loss_rpn_loc: 0.1753    time: 0.3060  last_time: 0.2196  data_time: 0.0085  last_data_time: 0.0158   lr: 0.00020739  max_mem: 1422M\n",
            "[02/22 06:36:40 d2.utils.events]:  eta: 0:00:55  iter: 279  total_loss: 1.167  loss_cls: 0.2387  loss_box_reg: 0.468  loss_mask: 0.2279  loss_rpn_cls: 0.03253  loss_rpn_loc: 0.1831    time: 0.3051  last_time: 0.2672  data_time: 0.0420  last_data_time: 0.0139   lr: 0.00022338  max_mem: 1424M\n",
            "[02/22 06:36:45 d2.utils.events]:  eta: 0:00:49  iter: 299  total_loss: 1.059  loss_cls: 0.2192  loss_box_reg: 0.4734  loss_mask: 0.2013  loss_rpn_cls: 0.02626  loss_rpn_loc: 0.1654    time: 0.3000  last_time: 0.2205  data_time: 0.0077  last_data_time: 0.0029   lr: 0.00023936  max_mem: 1424M\n",
            "[02/22 06:36:50 d2.utils.events]:  eta: 0:00:44  iter: 319  total_loss: 1.108  loss_cls: 0.2041  loss_box_reg: 0.4578  loss_mask: 0.2093  loss_rpn_cls: 0.04184  loss_rpn_loc: 0.1657    time: 0.2970  last_time: 0.2791  data_time: 0.0178  last_data_time: 0.0107   lr: 0.00025534  max_mem: 1424M\n",
            "[02/22 06:36:55 d2.utils.events]:  eta: 0:00:39  iter: 339  total_loss: 1.09  loss_cls: 0.1845  loss_box_reg: 0.4413  loss_mask: 0.2002  loss_rpn_cls: 0.01461  loss_rpn_loc: 0.1704    time: 0.2952  last_time: 0.1702  data_time: 0.0281  last_data_time: 0.0028   lr: 0.00027133  max_mem: 1424M\n",
            "[02/22 06:37:00 d2.utils.events]:  eta: 0:00:34  iter: 359  total_loss: 1.035  loss_cls: 0.2089  loss_box_reg: 0.4366  loss_mask: 0.2138  loss_rpn_cls: 0.02246  loss_rpn_loc: 0.1587    time: 0.2925  last_time: 0.2785  data_time: 0.0136  last_data_time: 0.0098   lr: 0.00028731  max_mem: 1424M\n",
            "[02/22 06:37:06 d2.utils.events]:  eta: 0:00:29  iter: 379  total_loss: 1.172  loss_cls: 0.2134  loss_box_reg: 0.451  loss_mask: 0.2094  loss_rpn_cls: 0.02793  loss_rpn_loc: 0.1786    time: 0.2924  last_time: 0.3054  data_time: 0.0270  last_data_time: 0.0067   lr: 0.0003033  max_mem: 1759M\n",
            "[02/22 06:37:11 d2.utils.events]:  eta: 0:00:24  iter: 399  total_loss: 0.9835  loss_cls: 0.1944  loss_box_reg: 0.3944  loss_mask: 0.1762  loss_rpn_cls: 0.02806  loss_rpn_loc: 0.1609    time: 0.2895  last_time: 0.1717  data_time: 0.0164  last_data_time: 0.0026   lr: 0.00031928  max_mem: 1759M\n",
            "[02/22 06:37:15 d2.utils.events]:  eta: 0:00:19  iter: 419  total_loss: 1.079  loss_cls: 0.2061  loss_box_reg: 0.4516  loss_mask: 0.1961  loss_rpn_cls: 0.02901  loss_rpn_loc: 0.1879    time: 0.2866  last_time: 0.3491  data_time: 0.0161  last_data_time: 0.0685   lr: 0.00033526  max_mem: 1759M\n",
            "[02/22 06:37:21 d2.utils.events]:  eta: 0:00:14  iter: 439  total_loss: 0.9652  loss_cls: 0.1736  loss_box_reg: 0.4137  loss_mask: 0.1933  loss_rpn_cls: 0.01593  loss_rpn_loc: 0.1665    time: 0.2872  last_time: 0.2934  data_time: 0.0393  last_data_time: 0.0032   lr: 0.00035125  max_mem: 1759M\n",
            "[02/22 06:37:26 d2.utils.events]:  eta: 0:00:09  iter: 459  total_loss: 0.9568  loss_cls: 0.152  loss_box_reg: 0.382  loss_mask: 0.1778  loss_rpn_cls: 0.02193  loss_rpn_loc: 0.1823    time: 0.2850  last_time: 0.2208  data_time: 0.0165  last_data_time: 0.0576   lr: 0.00036723  max_mem: 1759M\n",
            "[02/22 06:37:31 d2.utils.events]:  eta: 0:00:04  iter: 479  total_loss: 0.9627  loss_cls: 0.1652  loss_box_reg: 0.3928  loss_mask: 0.1778  loss_rpn_cls: 0.01865  loss_rpn_loc: 0.1952    time: 0.2830  last_time: 0.4222  data_time: 0.0155  last_data_time: 0.1792   lr: 0.00038322  max_mem: 1759M\n",
            "[02/22 06:37:38 d2.utils.events]:  eta: 0:00:00  iter: 499  total_loss: 0.9735  loss_cls: 0.1751  loss_box_reg: 0.4143  loss_mask: 0.1864  loss_rpn_cls: 0.0148  loss_rpn_loc: 0.168    time: 0.2831  last_time: 0.1924  data_time: 0.0462  last_data_time: 0.0022   lr: 0.0003992  max_mem: 1759M\n",
            "[02/22 06:37:38 d2.engine.hooks]: Overall training speed: 498 iterations in 0:02:21 (0.2831 s / it)\n",
            "[02/22 06:37:38 d2.engine.hooks]: Total training time: 0:02:27 (0:00:06 on hooks)\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/scan_org_charts/data/Org_chart/learning\"\n",
        "path_train = path + \"/train\"\n",
        "path_coco = path + \"/Organization_annotation.json\"\n",
        "path_data = \"/content/drive/MyDrive/scan_org_charts/data/Org_chart/cropped\"\n",
        "# set train data\n",
        "register_coco_instances(\"org_chart_train\", {}, path_coco, path_train)\n",
        "\n",
        "# setting for using the model\n",
        "cfg = get_cfg() # initialize\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"org_chart_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 1\n",
        "cfg.SOLVER.BASE_LR = 0.0004\n",
        "cfg.SOLVER.MAX_ITER = (500)\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (128)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
        "\n",
        "# train\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True) # for output\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change the test data form for dectron2\n",
        "DatasetCatalog.register(\"org\", lambda: get_test_dicts(path_data))\n",
        "MetadataCatalog.get(\"org\").set(thing_classes=[\"department\"])"
      ],
      "metadata": {
        "id": "_kvOOpS1aONS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBZjAShvKg3r",
        "outputId": "e039a21f-1a53-4ecf-afab-e8b675ab277d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/22 06:48:11 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\n"
          ]
        }
      ],
      "source": [
        "# the function for making the meta-data dict of the test data\n",
        "def get_test_dicts(img_dir):\n",
        "    img_files = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "    dataset_dicts = []\n",
        "    for idx, img_file in enumerate(img_files):\n",
        "        record = {}\n",
        "        record[\"file_name\"] = img_file\n",
        "        record[\"image_id\"] = idx\n",
        "        record[\"height\"], record[\"width\"] = cv2.imread(img_file).shape[:2]\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # load trained weights\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # score\n",
        "cfg.DATASETS.TEST = (\"org\", )  # set the test data to the model\n",
        "\n",
        "# detect departments\n",
        "predictor = DefaultPredictor(cfg)\n",
        "metadata = MetadataCatalog.get(\"org\")\n",
        "dataset_dicts = DatasetCatalog.get(\"org\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rRuY2farKg3r",
        "outputId": "c116e95e-7a4a-4336-b774-c7a38f6c2130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2091 [00:07<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fcf7555c38f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mjson_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mjson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "output_path = \"/content/drive/MyDrive/scan_org_charts/data/Org_chart/output\"\n",
        "for d in tqdm(dataset_dicts):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(img)\n",
        "    json_output = {\n",
        "    \"file_name\": d[\"file_name\"],\n",
        "    \"pred_boxes\": outputs[\"instances\"].pred_boxes.tensor.cpu().numpy().tolist(),\n",
        "    \"scores\": outputs[\"instances\"].scores.cpu().numpy().tolist(),\n",
        "    \"pred_classes\": outputs[\"instances\"].pred_classes.cpu().numpy().tolist(),\n",
        "    \"num_department\": len(outputs[\"instances\"].pred_boxes.tensor.cpu().numpy().tolist())\n",
        "    }\n",
        "    # save JSON\n",
        "    base_name = os.path.basename(d[\"file_name\"])\n",
        "    json_name = base_name.replace(\".png\", \".json\")\n",
        "    json_path = os.path.join(output_path, json_name)\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(json_output, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPCPJeJVby-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}